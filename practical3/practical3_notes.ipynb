{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows us to reset the Tensorflow network (computation graph)\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Parameters\n",
    "\n",
    "Example from lectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  0.9\n",
      "Result:  8.54\n",
      "Result:  13.124001\n",
      "Result:  15.874401\n",
      "Result:  17.52464\n",
      "Result:  18.514786\n",
      "Result:  19.108871\n",
      "Result:  19.46532\n",
      "Result:  19.679192\n",
      "Result:  19.807514\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, name=\"target\")\n",
    "learning_rate = tf.placeholder(\n",
    "    tf.float32,\n",
    "    name=\"learning_rate\")\n",
    "\n",
    "W = tf.get_variable(\"W\", initializer=[0.2, 0.7])\n",
    "y = tf.reduce_sum(x * W)\n",
    "\n",
    "loss = tf.pow(target - y, 2.0)\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        result, _ = sess.run(\n",
    "            [y, train_op], \n",
    "            feed_dict={x: [1.0, 1.0], \n",
    "                       target: 20.0, \n",
    "                       learning_rate: 0.1}) \n",
    "        print(\"Result: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Layers\n",
    "\n",
    "Instead of manually creating the trainable variables, we can use the feedforward layer.\n",
    "\n",
    "This creates a hidden layer that takes x as input, has 1 output neuron and no non-linear activation. The parameters used to connect the two layers together are created automatically and are trained during optimization. \n",
    "\n",
    "Here we do that and replace the manually created variables with a Tensorflow dense layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result:  -0.54732126\n",
      "Result:  11.781071\n",
      "Result:  16.71243\n",
      "Result:  18.68497\n",
      "Result:  19.473988\n",
      "Result:  19.789597\n",
      "Result:  19.91584\n",
      "Result:  19.966335\n",
      "Result:  19.986534\n",
      "Result:  19.994614\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, name=\"target\")\n",
    "learning_rate = tf.placeholder(\n",
    "    tf.float32, \n",
    "    name=\"learning_rate\")\n",
    "\n",
    "y = tf.layers.dense(x, 1, activation=None)\n",
    "\n",
    "loss = tf.pow(target - y, 2.0)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate=learning_rate)\n",
    "\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(10):\n",
    "        result, _ = sess.run(\n",
    "            [y, train_op], \n",
    "            feed_dict={x: [[1.0, 1.0]], \n",
    "                       target: 20.0, \n",
    "                       learning_rate: 0.1}) \n",
    "        print(\"Result: \", result[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution actually converges slightly faster because it is also internally creating a bias parameter.\n",
    "\n",
    "In large networks, you would normally chain together many large layers with **non-linear** activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 300], name=\"x\")\n",
    "hidden1 = tf.layers.dense(x, 100, activation=tf.tanh)\n",
    "hidden2 = tf.layers.dense(hidden1, 50, activation=tf.tanh)\n",
    "y = tf.layers.dense(hidden2, 1, activation=tf.sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "The sigmoid function, also known as the logistic function, is the most classic non-linear activation. It transforms the value to a range between 0 and 1.\n",
    "\n",
    "In modern networks, the tanh function is used more often. It has more flexibility, as it transforms the input value to a range between -1 and 1, and can therefore output negative values as well.\n",
    "\n",
    "Another popular one is the **Rectified Linear Unit** function, or the ReLU. This function acts as a linear function above zero, but restricts everything below zero to 0. Doing this, it also introduces a non-linearity. The partial linear property of the relu can help it converge faster on some tasks, although in practice I've found tanh to be a more robust option.\n",
    "\n",
    "Finally, softmax is a special type of activation function. It takes a whole layer as input and converts it into a probability distribution, such that all values are between 0 and 1, and together they sum up to 1. It is often used in the output layers of networks when performing classification, in order to predict a probability distribution over all the possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "hidden = tf.layers.dense(x, 100, activation=tf.sigmoid)\n",
    "\n",
    "# tanh\n",
    "hidden = tf.layers.dense(x, 100, activation=tf.tanh)\n",
    "\n",
    "# ReLU\n",
    "hidden = tf.layers.dense(x, 100, activation=tf.nn.relu)\n",
    "\n",
    "# softmax\n",
    "output = tf.layers.dense(hidden, 2, activation=None)\n",
    "probabilities = tf.nn.softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operations and Useful Functions\n",
    "\n",
    "Tensorflow has corresponding versions of all the main operations you might want to use. This means you can add them into your computation graph and into your neural network.\n",
    "\n",
    "We have operators for scalar values, but also to vectors, matrices and higher-order tensors (applied element-wise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.gen_math_ops.exp(x, name=None)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.abs # absolute value\n",
    "tf.negative # computes the negative value\n",
    "tf.sign # returns 1, 0 or -1 depending on the sign of the input\n",
    "tf.reciprocal # reciprocal 1/x\n",
    "tf.square # return input squared\n",
    "tf.round # return rounded value\n",
    "tf.sqrt # square root\n",
    "tf.rsqrt # reciprocal of square root\n",
    "tf.pow # power\n",
    "tf.exp # exponential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also some operators are performed over a whole vector/ matrix tensor and return a single value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.math_ops.argmin(input, axis=None, name=None, dimension=None, output_type=tf.int64)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum # Add elements together\n",
    "tf.reduce_mean # Average over elements\n",
    "tf.reduce_min # Minimum value\n",
    "tf.reduce_max # Maximum value\n",
    "tf.argmax # Index of the largest value\n",
    "tf.argmin # Index of the smallest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different adaptive learning rate strategies are also implemented in Tensorflow as functions. The main ones to try are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.training.adam.AdamOptimizer"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.GradientDescentOptimizer\n",
    "tf.train.AdadeltaOptimizer\n",
    "tf.train.AdamOptimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an XOR Function\n",
    "\n",
    "XOR is the function that takes two binary values and returns 1 only if one of them is 1 and the other 0, while returning 0 if both of them have the same value.\n",
    "\n",
    "It can be a complicated function to optimize and cannot be modeled with a linear model. But let's try anyway.\n",
    "\n",
    "Our dataset consists of all the possible different states that XOR can take:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we construct a linear network and optimize it on this dataset, printing out the predictions at each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 :  0.0\t1.2826594\t-0.6343475\t0.6483119\n",
      "Epoch  10 :  0.38700476\t0.585186\t0.37934893\t0.57753015\n",
      "Epoch  20 :  0.47045797\t0.5064148\t0.4843132\t0.52027\n",
      "Epoch  30 :  0.4922764\t0.49997452\t0.4976014\t0.5052995\n",
      "Epoch  40 :  0.4979807\t0.49981052\t0.4995557\t0.5013855\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "y = tf.reduce_sum(tf.layers.dense(x, 1, activation=None), axis=1)\n",
    "\n",
    "loss = tf.reduce_sum(tf.pow(target - y, 2.0))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]\n",
    "lr = 0.1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not doing very well. Ideally, the predictions should be [0, 1, 1, 0], but in this case they are hovering around 0.5 for every input case.\n",
    "\n",
    "In order to improve this architecture, let's add some non-linear layers into our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 :  0.5\t0.36924797\t0.48300314\t0.3753078\n",
      "Epoch  10 :  0.35206163\t0.5785061\t0.5704008\t0.491058\n",
      "Epoch  20 :  0.12587155\t0.65398777\t0.64453834\t0.22893003\n",
      "Epoch  30 :  0.08895578\t0.852751\t0.8530928\t0.17340218\n",
      "Epoch  40 :  0.06627951\t0.8881273\t0.893137\t0.12493227\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.float32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "hidden = tf.layers.dense(x, 5, activation=tf.tanh) # <- non-linear layer\n",
    "y = tf.reduce_sum(tf.layers.dense(hidden, 1, activation=tf.sigmoid), axis=1) # <- non-linear layer\n",
    "\n",
    "loss = tf.reduce_sum(tf.pow(target - y, 2.0))\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_y = [0.0, 1.0, 1.0, 0.0]\n",
    "lr = 1.0\n",
    "\n",
    "tf.set_random_seed(20)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([y, train_op], feed_dict={x: data_x, target: data_y, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \"\\t\".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better. The values are much closer to [0, 1, 1, 0] than before, and they will continue improving if we train for longer.\n",
    "\n",
    "We also had to increase the learning rate for this network. It was still learning with the smaller learning rate, but was convering very slowly. Remember, learning rate is a hyperparameter that can vary quite a bit depending on the network architecture and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR Classification\n",
    "\n",
    "We can also do classification with Tensorflow. For this, we often use the softmax activation function described above, which predicts the probability for each of the possible classes.\n",
    "\n",
    "We also have to change the loss function, as squared error is not suitable for classification. The loss function that works best with softmax is cross entropy. When minimizing cross entropy, we are essentially minimizing the negative log likelihood of the correct class for each datapoint. That's exactly what we want, as the model learns to assign high values for the correct label.\n",
    "\n",
    "We can change the XOR example above to perform classification instead. In this case, we are constructing a binary classifier - choosing between the classes of 0 and 1. When printing the output, we are printing the predicted classes, which were assigned the highest probability by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  0 :  0 0 1 0\n",
      "Epoch  10 :  0 0 1 1\n",
      "Epoch  20 :  0 1 1 1\n",
      "Epoch  30 :  0 1 1 0\n",
      "Epoch  40 :  0 1 1 0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
    "target = tf.placeholder(tf.int32, [None], name=\"target\")\n",
    "learning_rate = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "\n",
    "hidden = tf.layers.dense(x, 5, activation=tf.tanh)\n",
    "output = tf.layers.dense(hidden, 2, activation=None)\n",
    "\n",
    "probabilities = tf.nn.softmax(output) # moved to softmax activation function\n",
    "predictions = tf.argmax(probabilities, axis=1)\n",
    "loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=target)\n",
    "loss = tf.reduce_mean(loss_) # moved to cross entropy loss function\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss)\n",
    "\n",
    "data_x = [[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]]\n",
    "data_targets = [0, 1, 1, 0]\n",
    "lr = 1.0\n",
    "\n",
    "tf.set_random_seed(20)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(50):\n",
    "        result, _ = sess.run([predictions, train_op], feed_dict={x: data_x, target: data_targets, learning_rate: lr})\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Epoch \", epoch, \": \", \" \".join([str(x) for x in result]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the model starts off with incorrect predictions, but fairly soon learns to return the correct sequence of [0, 1, 1, 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
